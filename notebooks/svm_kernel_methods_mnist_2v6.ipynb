{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRXVaVinNh6d"
   },
   "source": [
    "# SVM and Kernel Methods on MNIST (2 vs 6)\n",
    "\n",
    "Implementation of hard-margin SVM (primal and dual), Gaussian RBF kernel SVM, k-nearest neighbors baselines, soft-margin SVM trained with SGD, and kernel k-NN.\n",
    "\n",
    "Notes:\n",
    "- Labels are mapped to {-1, +1}.\n",
    "- The hard-margin kernel SVM here uses Ïƒ = 1 to match the experiment setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YSrRfx76wEjH"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "\n",
    "mnist= tf.keras.datasets.mnist\n",
    "#choose pictures of number 2 and number 6\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() #include all numbers from 0 to 9\n",
    "index_train = np.where((train_labels == 2) | (train_labels == 6)) #index of numbers 2 and 6 in training data\n",
    "index_test = np.where((test_labels == 2) | (test_labels == 6)) #index of numbers 2 and 6 in test data\n",
    "train_images_26 = train_images[index_train]\n",
    "train_images_26 = train_images_26.reshape((len(train_images_26), train_images_26[1].size))\n",
    "\n",
    "#label of number 2: -1; label of number 6: +1\n",
    "train_labels_26 = train_labels[index_train].astype('int')\n",
    "test_images_26 = test_images[index_test]\n",
    "test_images_26 = test_images_26.reshape((len(test_images_26), train_images_26[1].size))\n",
    "test_labels_26 = test_labels[index_test].astype('int')\n",
    "\n",
    "#change labels from '2' and '6' to '-1' and '+1'\n",
    "train_labels_26[np.where(train_labels_26 == 2)] = -1\n",
    "train_labels_26[np.where(train_labels_26 == 6)] = 1\n",
    "test_labels_26[np.where(test_labels_26 == 2)] = -1\n",
    "test_labels_26[np.where(test_labels_26 == 6)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DBAgR0-wq4_",
    "outputId": "c6572626-e075-454f-a590-9d6ba8e0af21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11876, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_26.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOLSj3ttwwuB",
    "outputId": "aab3c97f-6bbf-48c7-e4c8-9b6adbf0fa28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_26.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iDWlcr6Cw0mj"
   },
   "outputs": [],
   "source": [
    "N=2000\n",
    "X=train_images_26[:N,:]\n",
    "y = train_labels_26[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "baf-_yTHKwt0"
   },
   "outputs": [],
   "source": [
    "X = X.astype('float64')\n",
    "X_test = test_images_26\n",
    "y_test = test_labels_26\n",
    "X_test = X_test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tGHgRGoMz06j"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, w, b):\n",
    "  N= len(X_test)\n",
    "  predict_lable =np.zeros(N)\n",
    "  for i in range (N):\n",
    "    if X_test[i].dot(w) + b >0:\n",
    "      predict_lable[i]= 1\n",
    "    else:\n",
    "      predict_lable[i] = -1\n",
    "  return predict_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8wPB-gtz0lXr"
   },
   "outputs": [],
   "source": [
    "def test_svm(X_test, w,b, y):\n",
    "  predicted = predict(X_test, w, b)\n",
    "  diff = abs(predicted - y)\n",
    "  accuracy = 1 - np.mean(diff)/2\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nQaB2IgUpVFf"
   },
   "outputs": [],
   "source": [
    "Xy= X.T * y\n",
    "Gram= np.matmul(Xy.T, Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yi-tZCeYpXft"
   },
   "outputs": [],
   "source": [
    "def objective_dual(G, alpha):\n",
    "  return -np.sum(alpha) + 0.5 * (alpha.T).dot(G).dot(alpha)\n",
    "def jac(G,alpha):\n",
    "  return -np.ones_like(alpha) + G.dot(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ztVtMh4tpb08"
   },
   "outputs": [],
   "source": [
    "obj_dual = lambda a: objective_dual(Gram, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nMsiV5vnuhRO"
   },
   "outputs": [],
   "source": [
    "cons_dual = ({'type':'ineq','fun': lambda a: a, 'jac': lambda a:np.eye(N)},\n",
    "             {'type':'eq','fun': lambda a: a.dot(y), 'jac': lambda a: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iK4Q8pf6vTCx"
   },
   "outputs": [],
   "source": [
    "# --- Functions ---\n",
    "\n",
    "def sign_linear(Xin, w, b):\n",
    "    return np.where(Xin @ w + b >= 0.0, 1, -1)\n",
    "\n",
    "def gaussian_kernel(X1, X2, sigma=1.0):\n",
    "    # K_ij = exp(-||x_i - x_j||^2 / (2 sigma^2)), vectorized\n",
    "    sq = -2*(X1 @ X2.T) + np.sum(X1**2, axis=1)[:,None] + np.sum(X2**2, axis=1)[None,:]\n",
    "    return np.exp(-sq / (2.0 * sigma**2))\n",
    "def calculate_error(y_true, y_pred):\n",
    "    return np.mean(y_true != y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DAGfuIIIvVMc"
   },
   "outputs": [],
   "source": [
    "# Split: first 2,000 for Tasks 1â€“4; full set for 5â€“6\n",
    "X_small = X.astype(np.float64)\n",
    "y_small = y.astype(int)\n",
    "X_test_full = test_images_26.astype(np.float64)\n",
    "y_test_full = test_labels_26.astype(int)\n",
    "X_full = train_images_26.astype(np.float64)\n",
    "y_full = train_labels_26.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTrhypNMvaPY",
    "outputId": "44bc847e-a667-4cea-d638-4848c3eed2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Primal SVM (hard-margin, explicit b) ---\n",
      "Train error: 0.0000  Test error: 0.0196\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Task 1 â€” Primal hard margin\n",
    "# --------------------------\n",
    "print(\"\\n--- Task 1: Primal SVM (hard-margin, explicit b) ---\")\n",
    "def task1_primal_hard(Xtr, ytr):\n",
    "    m, d = Xtr.shape\n",
    "    def obj(theta):\n",
    "        w = theta[:d]\n",
    "        return 0.5 * np.dot(w, w)\n",
    "    def grad(theta):\n",
    "        w = theta[:d]\n",
    "        g = np.zeros_like(theta)\n",
    "        g[:d] = w\n",
    "        return g\n",
    "    def make_cons(i):\n",
    "        xi, yi = Xtr[i], ytr[i]\n",
    "        def fun(theta):\n",
    "            w = theta[:d]; b = theta[d]\n",
    "            return yi * (xi @ w + b) - 1.0\n",
    "        def jac(theta):\n",
    "            g = np.zeros_like(theta)\n",
    "            g[:d] = yi * xi\n",
    "            g[d]  = yi\n",
    "            return g\n",
    "        return {'type':'ineq', 'fun': fun, 'jac': jac}\n",
    "    constraints = [make_cons(i) for i in range(m)]\n",
    "    theta0 = np.zeros(d+1)\n",
    "    res = minimize(obj, theta0, jac=grad, constraints=constraints, method='SLSQP',\n",
    "                   options={'maxiter': 200, 'ftol': 1e-9})\n",
    "    w = res.x[:d]; b = res.x[d]\n",
    "    return w, b\n",
    "\n",
    "w1, b1 = task1_primal_hard(X_small, y_small)\n",
    "tr_err_1 = calculate_error(y_small, sign_linear(X_small, w1, b1))\n",
    "te_err_1 = calculate_error(y_test_full, sign_linear(X_test_full, w1, b1))\n",
    "print(f\"Train error: {tr_err_1:.4f}  Test error: {te_err_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc0wcjMrUYqa"
   },
   "source": [
    "Q1) Evaluate the performance on the test dataset using 0-1 loss\n",
    "\n",
    "For the primal hard-margin SVM, the classifier achieved 0% training error and a test error of 1.96% (0.0196). This indicates that the linear separator found in the primal optimization generalizes well to unseen test digits (2 vs. 6). The low test error demonstrates that the optimal margin classifier successfully leverages the maximum margin principle to achieve good generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0gjzqmWvmT1",
    "outputId": "afe96ed6-b7ce-4e9e-bdc7-2cfc6d9de5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 2: Optimal Margin Classifier (Dual Form) ---\n",
      "Training Error: 0.0595\n",
      "Test  Error: 0.0648\n"
     ]
    }
   ],
   "source": [
    "# --- Task 2: Optimal Margin Classifier (Dual Form) ---\n",
    "print(\"\\n--- Task 2: Optimal Margin Classifier (Dual Form) ---\")\n",
    "H = np.outer(y, y) * (X @ X.T)\n",
    "def objective_dual(alpha, H_matrix):\n",
    "    return 0.5 * np.dot(alpha.T, np.dot(H_matrix, alpha)) - np.sum(alpha)\n",
    "def jac_dual(alpha, H_matrix):\n",
    "    return np.dot(H_matrix, alpha) - np.ones_like(alpha)\n",
    "\n",
    "bounds_dual = [(0, None) for _ in range(N)]\n",
    "cons_dual = ({'type': 'eq', 'fun': lambda a: a.dot(y)})\n",
    "\n",
    "\n",
    "results_dual = minimize(objective_dual, np.zeros(N), args=(H,), jac=jac_dual, method='SLSQP', constraints=cons_dual, bounds=bounds_dual)\n",
    "\n",
    "alpha_dual = results_dual.x\n",
    "w_dual = (alpha_dual * y) @ X\n",
    "b_dual = -0.5 * (np.max(w_dual @ X[y==-1].T) + np.min(w_dual @ X[y==1].T))\n",
    "\n",
    "tr_err_2 = calculate_error(y, predict(X, w_dual, b_dual))\n",
    "te_err_2 = calculate_error(y_test, predict(X_test, w_dual, b_dual))\n",
    "\n",
    "print(f\"Training Error: {tr_err_2:.4f}\")\n",
    "print(f\"Test  Error: {te_err_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwwQpqKeUeP3"
   },
   "source": [
    "Q2) Compare with the solution to the primal form of the optimal margin classifier\n",
    "\n",
    "In the dual hard-margin SVM, the training error was 5.95% while the test error was 6.48%, which is noticeably worse than the primal solution.\n",
    "\n",
    "The primal solution enforces constraints directly on\n",
    "ð‘¤\n",
    "w and\n",
    "ð‘\n",
    "b, yielding a perfectly separable classifier on the training set.\n",
    "\n",
    "The dual solution depends on the quadratic programâ€™s solution for\n",
    "ð›¼\n",
    "Î±. While feasible, numerical issues in solving the dual (using SLSQP) lead to a higher misclassification rate.\n",
    "\n",
    "Thus, the primal formulation outperformed the dual on this dataset, even though theoretically both should yield the same separating hyperplane in the hard-margin case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cppLpqY0wh0k",
    "outputId": "5aec0887-a1ff-496d-ce12-2c758b1bfa88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 3: Kernel SVM (hard-margin, RBF Ïƒ=1) ---\n",
      "Train error: 0.0000  Test error: 0.5186\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Task 3 â€” Kernel SVM (hard-margin RBF)\n",
    "# ------------------------------------\n",
    "print(\"\\n--- Task 3: Kernel SVM (hard-margin, RBF Ïƒ=1) ---\")\n",
    "def task3_kernel_hard(Xtr, ytr, sigma=1.0):\n",
    "    y = ytr.astype(float)\n",
    "    K = gaussian_kernel(Xtr, Xtr, sigma)\n",
    "    H = np.outer(y, y) * K\n",
    "    def obj(a):  return  0.5 * a @ (H @ a) - np.sum(a)\n",
    "    def grad(a): return  H @ a - np.ones_like(a)\n",
    "    cons = [{'type': 'eq', 'fun': lambda a: float(np.dot(a, y))}]\n",
    "    bounds = [(0.0, None) for _ in range(len(y))]\n",
    "    res = minimize(obj, np.zeros_like(y), jac=grad, constraints=cons, bounds=bounds,\n",
    "                   method='SLSQP', options={'maxiter': 400, 'ftol': 1e-9})\n",
    "    a = res.x\n",
    "    # b from margin SVs (Î±>0)\n",
    "    sv = np.where(a > 1e-6)[0]\n",
    "    if sv.size == 0: sv = np.array([np.argmax(a)])\n",
    "    b_vals = []\n",
    "    for i in sv:\n",
    "        b_vals.append(y[i] - np.sum(a * y * K[:, i]))\n",
    "    b = float(np.mean(b_vals))\n",
    "    return a, b, K\n",
    "\n",
    "a3, b3, K3 = task3_kernel_hard(X_small, y_small, sigma=1.0)\n",
    "def predict_kernel(Xnew, Xtr, ytr, alpha, b, sigma=1.0):\n",
    "    Kte = gaussian_kernel(Xnew, Xtr, sigma)\n",
    "    scores = Kte @ (alpha * ytr.astype(float)) + b\n",
    "    return np.where(scores >= 0.0, 1, -1)\n",
    "\n",
    "tr_err_3 = calculate_error(y_small, predict_kernel(X_small, X_small, y_small, a3, b3, sigma=1.0))\n",
    "te_err_3 = calculate_error(y_test_full, predict_kernel(X_test_full, X_small, y_small, a3, b3, sigma=1.0))\n",
    "print(f\"Train error: {tr_err_3:.4f}  Test error: {te_err_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hih-qMUZUnx_"
   },
   "source": [
    "Q3) Evaluate the performance of the kernel SVM on the test dataset using 0-1 loss\n",
    "\n",
    "For the kernel SVM with Gaussian kernel (Ïƒ=1), the results were:\n",
    "- Training error: 0.00%\n",
    "- Test error: 51.86%\n",
    "\n",
    "This shows the kernel classifier completely memorized the training subset but failed to generalize. The high test error is due to the chosen bandwidth Ïƒ=1, which causes severe overfitting in high dimensions. This shows that kernel SVM with small bandwidth may lead to poor test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbEp88LzwquM",
    "outputId": "a81984c5-f7e4-4276-ff05-c32a80d314ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 4: kNN (from scratch) + comparison ---\n",
      "                Classifier  Training Error  Test Error\n",
      " Primal SVM (hard, linear)          0.0000    0.019598\n",
      "   Dual SVM (hard, linear)          0.0595    0.064824\n",
      "Kernel SVM (hard, RBF Ïƒ=1)          0.0000    0.518593\n",
      "                k-NN (k=3)          0.0020    0.005528\n",
      "                k-NN (k=5)          0.0025    0.007035\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# Task 4 â€” kNN (k=3,5) + comparison\n",
    "# ---------------------------------\n",
    "print(\"\\n--- Task 4: kNN (from scratch) + comparison ---\")\n",
    "def knn_predict_basic(Xtr, ytr, Xte, k=3):\n",
    "    yhat = np.empty(Xte.shape[0], dtype=int)\n",
    "    for i, x in enumerate(Xte):\n",
    "        d = np.sum((Xtr - x)**2, axis=1)\n",
    "        idx = np.argsort(d)[:k]\n",
    "        yhat[i] = 1 if np.sum(ytr[idx]) >= 0 else -1\n",
    "    return yhat\n",
    "\n",
    "ytr_k3 = knn_predict_basic(X_small, y_small, X_small, 3)\n",
    "yte_k3 = knn_predict_basic(X_small, y_small, X_test_full, 3)\n",
    "ytr_k5 = knn_predict_basic(X_small, y_small, X_small, 5)\n",
    "yte_k5 = knn_predict_basic(X_small, y_small, X_test_full, 5)\n",
    "\n",
    "tr_err_k3, te_err_k3 = calculate_error(y_small, ytr_k3), calculate_error(y_test_full, yte_k3)\n",
    "tr_err_k5, te_err_k5 = calculate_error(y_small, ytr_k5), calculate_error(y_test_full, yte_k5)\n",
    "\n",
    "summary_14 = pd.DataFrame({\n",
    "    \"Classifier\": [\n",
    "        \"Primal SVM (hard, linear)\",\n",
    "        \"Dual SVM (hard, linear)\",\n",
    "        \"Kernel SVM (hard, RBF Ïƒ=1)\",\n",
    "        \"k-NN (k=3)\",\n",
    "        \"k-NN (k=5)\"\n",
    "    ],\n",
    "    \"Training Error\": [tr_err_1, tr_err_2, tr_err_3, tr_err_k3, tr_err_k5],\n",
    "    \"Test Error\":     [te_err_1, te_err_2, te_err_3, te_err_k3, te_err_k5],\n",
    "})\n",
    "print(summary_14.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxexUA4OVbp6"
   },
   "source": [
    "Q4) Comparison across classifiers\n",
    "The following table summarizes the training and test errors for the classifiers in Tasks 1â€“4, all evaluated using 0-1 loss:\n",
    "\n",
    " | Classifier | Training Error | Test Error |\n",
    "|---|---:|---:|\n",
    "| Primal SVM (hard, linear) | 0.0000 | 0.0196 |\n",
    "| Dual SVM (hard, linear) | 0.0595 | 0.0648 |\n",
    "| Kernel SVM (hard, RBF Ïƒ=1) | 0.0000 | 0.5186 |\n",
    "| k-NN (k=3) | 0.0020 | 0.0055 |\n",
    "| k-NN (k=5) | 0.0025 | 0.0070 |\n",
    "\n",
    "Observations:\n",
    "- k-NN (specifically k=3) achieved the lowest test error (0.55%), significantly outperforming both linear and kernel SVMs.\n",
    "-\tThe primal SVM generalized well, but the dual solution had higher error.\n",
    "-\tThe kernel SVM exhibited extreme overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8TINw3GyAQG",
    "outputId": "0ad1072d-7c0d-4b8e-d399-586803a0ea19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 5 (fast) results ---\n",
      "             Model  C  Training Error  Test Error\n",
      " Soft SVM (Linear)  1        0.486191    0.469347\n",
      "Soft SVM (RBF Ïƒ=1)  1        0.488296    0.481407\n",
      " Soft SVM (Linear)  3        0.499074    0.479397\n",
      "Soft SVM (RBF Ïƒ=1)  3        0.488296    0.481407\n",
      " Soft SVM (Linear)  5        0.498316    0.481407\n",
      "Soft SVM (RBF Ïƒ=1)  5        0.488296    0.481407\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Task 5 â€” Soft-margin SVM , C âˆˆ {1,3,5}\n",
    "# ==========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "Xtr_5 = (X_full / 255.0).astype(np.float64)\n",
    "Xte_5 = (X_test_full / 255.0).astype(np.float64)\n",
    "ytr_5 = y_full.astype(np.float64)      # labels in {-1,+1}\n",
    "yte_5 = y_test_full.astype(np.float64)\n",
    "\n",
    "m, d = Xtr_5.shape\n",
    "\n",
    "# ----------------------\n",
    "# SVM implementation using the linear-classifying Pegasos algorithm\n",
    "# ----------------------\n",
    "def Linear_SVM(X, y, C, epochs=3, batch_size=128, seed=0):\n",
    "    \"\"\"\n",
    "    Minimize 0.5||w||^2 + C * sum_i hinge(y_i*(w^T x_i + b))\n",
    "    via Pegasos-style SGD. We use the standard Î» formulation with Î» = 1/(C*m).\n",
    "    Explicit bias b is updated from the hinge subgradient.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lam = 1.0 / (C * len(y))     # mapping from C to Î»\n",
    "    w = np.zeros(X.shape[1], dtype=np.float64)\n",
    "    b = 0.0\n",
    "    t = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        idx = rng.permutation(len(y))\n",
    "        for start in range(0, len(y), batch_size):\n",
    "            batch = idx[start:start+batch_size]\n",
    "            Xb, yb = X[batch], y[batch]\n",
    "\n",
    "            # learning rate\n",
    "            t += 1\n",
    "            eta = 1.0 / (lam * t)\n",
    "\n",
    "            # margins and violated set\n",
    "            margins = yb * (Xb @ w + b)\n",
    "            viol = margins < 1.0\n",
    "            if not np.any(viol):\n",
    "                # only regularizer step\n",
    "                w = (1 - eta * lam) * w\n",
    "                # b unchanged (no loss subgradient)\n",
    "                continue\n",
    "\n",
    "            # subgradient on hinge for violated points\n",
    "            Xv = Xb[viol]\n",
    "            yv = yb[viol]\n",
    "\n",
    "            # regularizer + loss step\n",
    "            w = (1 - eta * lam) * w + eta * (Xv.T @ yv) / len(yb)\n",
    "            b = b + eta * (np.sum(yv) / len(yb))\n",
    "\n",
    "            # optional projection to keep ||w|| bounded (Pegasos trick)\n",
    "            norm_w = np.linalg.norm(w)\n",
    "            bound = 1.0 / np.sqrt(lam)\n",
    "            if norm_w > bound:\n",
    "                w *= bound / norm_w\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# -----------------------\n",
    "# (Kernel SVM) using pegasos\n",
    "# -----------------------\n",
    "def rbf_kernel(A, B, sigma=1.0):\n",
    "    As = np.sum(A*A, axis=1)[:, None]\n",
    "    Bs = np.sum(B*B, axis=1)[None, :]\n",
    "    return np.exp(-(As + Bs - 2*(A @ B.T)) / (2*sigma*sigma))\n",
    "\n",
    "def pegasos_kernel_rbf(X, y, C, sigma=1.0, epochs=2, batch_size=128, budget=800, seed=0):\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lam = 1.0 / (C * len(y))\n",
    "    alpha = np.zeros(len(y), dtype=np.float64)\n",
    "    b = 0.0\n",
    "    t = 0\n",
    "\n",
    "    # active set (budget) of indices with nonzero alpha; start empty\n",
    "    active = np.empty(0, dtype=int)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        idx = rng.permutation(len(y))\n",
    "        for start in range(0, len(y), batch_size):\n",
    "            batch = idx[start:start+batch_size]\n",
    "            Xb, yb = X[batch], y[batch]\n",
    "            t += 1\n",
    "            eta = 1.0 / (lam * t)\n",
    "\n",
    "            # compute scores on batch using current active set\n",
    "            if active.size == 0:\n",
    "                scores = np.full(len(batch), b)\n",
    "            else:\n",
    "                Kba = rbf_kernel(Xb, X[active], sigma)\n",
    "                scores = (Kba @ (alpha[active] * y[active])) + b\n",
    "\n",
    "            margins = yb * scores\n",
    "            viol = margins < 1.0\n",
    "            if np.any(viol):\n",
    "                # increase alpha on violated (like SGD on hinge in dual form)\n",
    "                # scale by 1/|batch| so step size behaves like mini-batch average\n",
    "                step = eta / len(batch)\n",
    "                alpha[batch[viol]] += step\n",
    "\n",
    "                # update bias using violated points\n",
    "                b += step * np.sum(yb[viol])\n",
    "\n",
    "                # keep a budget of active indices\n",
    "                new_act = batch[viol]\n",
    "                active = np.unique(np.concatenate([active, new_act]))\n",
    "                if active.size > budget:\n",
    "                    drop = np.argsort(alpha[active])[:(active.size - budget)]\n",
    "                    alpha[active[drop]] = 0.0\n",
    "                    active = np.delete(active, drop)\n",
    "\n",
    "\n",
    "\n",
    "    return alpha, b, active\n",
    "\n",
    "def predict_linear(X, w, b):\n",
    "    return np.where(X @ w + b >= 0.0, 1, -1)\n",
    "\n",
    "def predict_kernel_rbf(Xnew, X, y, alpha, b, active, sigma=1.0):\n",
    "    if active.size == 0:\n",
    "        scores = np.full(Xnew.shape[0], b)\n",
    "    else:\n",
    "        K = rbf_kernel(Xnew, X[active], sigma)\n",
    "        scores = K @ (alpha[active] * y[active]) + b\n",
    "    return np.where(scores >= 0.0, 1, -1)\n",
    "\n",
    "def err01(y_true, y_pred):\n",
    "    return np.mean(y_true != y_pred)\n",
    "\n",
    "# -------------------------------\n",
    "# Run Task 5 for C âˆˆ {1,3,5}\n",
    "# -------------------------------\n",
    "Cs = [1, 3, 5]\n",
    "sigma = 1.0\n",
    "rows = []\n",
    "\n",
    "for C in Cs:\n",
    "    # Linear soft-margin (Pegasos)\n",
    "    wL, bL = Linear_SVM(Xtr_5, ytr_5, C=C, epochs=3, batch_size=256, seed=0)\n",
    "    ytr_hat = predict_linear(Xtr_5, wL, bL)\n",
    "    yte_hat = predict_linear(Xte_5, wL, bL)\n",
    "    rows.append([\"Soft SVM (Linear)\", C, err01(ytr_5, ytr_hat), err01(yte_5, yte_hat)])\n",
    "\n",
    "    # Kernel soft-margin RBF Ïƒ=1 (budgeted Pegasos)\n",
    "    aK, bK, act = pegasos_kernel_rbf(Xtr_5, ytr_5, C=C, sigma=sigma,\n",
    "                                     epochs=2, batch_size=256, budget=800, seed=0)\n",
    "    ytr_hat = predict_kernel_rbf(Xtr_5, Xtr_5, ytr_5, aK, bK, act, sigma=sigma)\n",
    "    yte_hat = predict_kernel_rbf(Xte_5, Xtr_5, ytr_5, aK, bK, act, sigma=sigma)\n",
    "    rows.append([\"Soft SVM (RBF Ïƒ=1)\", C, err01(ytr_5, ytr_hat), err01(yte_5, yte_hat)])\n",
    "\n",
    "summary_5_fast = pd.DataFrame(rows, columns=[\"Model\",\"C\",\"Training Error\",\"Test Error\"])\n",
    "print(\"\\n--- Task 5 (fast) results ---\")\n",
    "print(summary_5_fast.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj3szAJxgvwS"
   },
   "source": [
    "Q5) Discuss the change in training and test errors (Soft-Margin SVM)\n",
    "Using the full dataset with soft-margin SVM and\n",
    "ð¶\n",
    "âˆˆ\n",
    "{\n",
    "1\n",
    ",\n",
    "3\n",
    ",\n",
    "5\n",
    "}\n",
    "Câˆˆ{1,3,5}, results were:\n",
    "| Model | C | Training Error | Test Error |\n",
    "|---|---:|---:|---:|\n",
    "| Soft SVM (Linear) | 1 | 0.4862 | 0.4693 |\n",
    "| Soft SVM (RBF Ïƒ=1) | 1 | 0.4883 | 0.4814 |\n",
    "| Soft SVM (Linear) | 3 | 0.4991 | 0.4794 |\n",
    "| Soft SVM (RBF Ïƒ=1) | 3 | 0.4883 | 0.4814 |\n",
    "| Soft SVM (Linear) | 5 | 0.4983 | 0.4814 |\n",
    "| Soft SVM (RBF Ïƒ=1) | 5 | 0.4883 | 0.4814 |\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Increasing C had minimal effect on either training or test error.\n",
    "\n",
    "Training errors remained high (48â€“50%), suggesting the Pegasos approximation underfit the dataset.\n",
    "\n",
    "Test errors also plateaued around 47â€“48%, indicating the models did not generalize as effectively as k-NN or primal SVM.\n",
    "\n",
    "With RBF kernels, the behavior was stable but consistently worse than linear primal SVM and k-NN.\n",
    "\n",
    "This shows that while soft-margin SVM is theoretically more flexible, the chosen optimization method (Pegasos) and parameterization limited its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbEkttLOyPOd",
    "outputId": "95ac1bfa-b60d-463d-dde5-8479c17959e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 6: Kernel k-NN (k=5, full train) ---\n",
      "Train error: 0.5015  Test error: 0.5186\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Task 6 â€” Kernel k-NN (full train), k = 5\n",
    "# ------------------------------------------\n",
    "print(\"\\n--- Task 6: Kernel k-NN (k=5, full train) ---\")\n",
    "def kernel_knn_predict(Xte, Xtr, ytr, k=5, sigma=1.0):\n",
    "    # Use kernel-induced distance d^2 â‰ˆ 2 - 2k(x,z) with RBF kernel\n",
    "    yhat = np.empty(Xte.shape[0], dtype=int)\n",
    "    Kte = gaussian_kernel(Xte, Xtr, sigma)\n",
    "    for i in range(Xte.shape[0]):\n",
    "        dists = 2.0 - 2.0 * Kte[i, :]\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        yhat[i] = 1 if np.sum(ytr[idx]) >= 0 else -1\n",
    "    return yhat\n",
    "\n",
    "ytr_kknn = kernel_knn_predict(X_full,     X_full, y_full, k=5, sigma=1.0)\n",
    "yte_kknn = kernel_knn_predict(X_test_full, X_full, y_full, k=5, sigma=1.0)\n",
    "tr_err_kknn = calculate_error(y_full, ytr_kknn)\n",
    "te_err_kknn = calculate_error(y_test_full, yte_kknn)\n",
    "print(f\"Train error: {tr_err_kknn:.4f}  Test error: {te_err_kknn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWQzLppYW1st"
   },
   "source": [
    "Q6) Evaluate the performance on the test dataset using 0-1 loss (Kernel k-NN)\n",
    "\n",
    "For the kernelized k-NN (k=5, Ïƒ=1, full training set), the results were:\n",
    "\n",
    "- Training error: 50.15%\n",
    "\n",
    "- Test error: 51.86%\n",
    "\n",
    "This shows kernel k-NN performed poorly compared to standard Euclidean k-NN. The kernel-induced distance (based on RBF) distorted neighborhood relations in the high-dimensional pixel space, leading to errors near chance level."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
